{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcLUgq8lax5UU7N7fVx7jB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelgadda/LassoRegressionFromScratch/blob/main/LassoRegressionFromScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import lasso_path\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from copy import deepcopy\n"
      ],
      "metadata": {
        "id": "mF8kG1Ihhhqs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "RURWdrl7hbI7"
      },
      "outputs": [],
      "source": [
        "class NewLassoRegression:\n",
        "  def __init__(self, epochs=1000, lamb=1):\n",
        "    self.epochs = epochs\n",
        "    self.theta = None\n",
        "    self.intercept = None\n",
        "    self.lamb = lamb\n",
        "    self.update_steps=None\n",
        "\n",
        "  def preappend_bias_term(self, X):\n",
        "    inter = np.ones(X.shape[0])\n",
        "    X_new = np.column_stack((inter, X))\n",
        "    return X_new\n",
        "\n",
        "\n",
        "\n",
        "  def fit(self, X, Y):\n",
        "    #X = X / np.sqrt(np.sum(np.square(X), axis=0)) <-- Pre-normalizing the data\n",
        "    X = self.preappend_bias_term(X)\n",
        "\n",
        "    Y = Y.reshape(-1,1)\n",
        "    n_cols = X.shape[1]\n",
        "    n_rows = X.shape[0]\n",
        "\n",
        "    self.theta = np.zeros(n_cols).reshape(-1,1)\n",
        "    self.update_steps = np.zeros((10000, n_cols))\n",
        "    self.lamb = self.lamb * n_rows\n",
        "    dw = np.zeros(n_cols)\n",
        "    best_theta = np.zeros(n_cols)\n",
        "    for epoch in range(self.epochs):\n",
        "\n",
        "      y_pred = X @ self.theta\n",
        "      for column in range(n_cols):\n",
        "        X_col = X[:,column].reshape(-1,1)\n",
        "        theta_j = deepcopy(self.theta[column])\n",
        "\n",
        "        if column == 0:\n",
        "          temp_theta = self.theta[column+1:].reshape(-1,1)\n",
        "          temp_X = X[:,column+1:]\n",
        "        elif column + 1 == n_cols:\n",
        "          temp_theta = self.theta[:column].reshape(-1,1)\n",
        "          temp_X = X[:, :column]\n",
        "        else:\n",
        "          temp_theta = np.vstack((self.theta[:column], self.theta[column+1:])).reshape(-1,1)\n",
        "          temp_X = np.hstack((X[:, :column], X[:,column+1:]))\n",
        "\n",
        "        y_pred = temp_X @ temp_theta\n",
        "        rj = (Y - y_pred)\n",
        "        theta_k = (X_col.T @ rj)\n",
        "        if column != 0:\n",
        "          if theta_k < -self.lamb:\n",
        "\n",
        "            theta_k += self.lamb\n",
        "\n",
        "          elif theta_k >= -self.lamb and theta_k <= self.lamb:\n",
        "\n",
        "            theta_k = 0\n",
        "\n",
        "          else: #theta_j > lambda\n",
        "\n",
        "            theta_k -= self.lamb\n",
        "\n",
        "        self.update_steps[epoch, column] = theta_k / ((X_col.T @ X_col))\n",
        "        #Normalizing each update as we go instead of \"pre\"-normalizing the data\n",
        "        self.theta[column] = theta_k / ((X_col.T @ X_col))\n",
        "\n",
        "    self.theta[0] = np.sum(Y)/n_rows - self.theta[1:].T @ np.sum(X[:,1:], axis = 0)/n_rows\n",
        "\n",
        "  def predict(self, X, theta=None):\n",
        "      if theta is None:\n",
        "        theta = self.theta\n",
        "      return X@theta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_dataset = datasets.load_diabetes()\n",
        "\n",
        "# Access the data and target variables\n",
        "X = diabetes_dataset.data[:, :8]\n",
        "y = diabetes_dataset.target.reshape(-1,1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 99)"
      ],
      "metadata": {
        "id": "BGV54NouhkvP"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#_, coef, _, n_iters = lasso_path(X_train, y_train, alphas=[1], return_n_iter = True ) <-- Lasso_path, using a different coordiante descent approach, it produced different results than linear_model.lasso\n",
        "LR = Lasso(alpha=1, fit_intercept=True)\n",
        "LR.fit(X_train, y_train)\n",
        "myLR = NewLassoRegression(lamb=1, epochs=1000)\n",
        "inter = np.ones(X_train.shape[0])\n",
        "inter_2 = np.ones(X_test.shape[0])\n",
        "X_new = np.column_stack((inter, X_train))\n",
        "X_test_new = np.column_stack((inter_2, X_test))\n",
        "myLR.fit(X_train, y_train)\n",
        "sklasso_mse = mean_squared_error(LR.predict(X_test), y_test)\n",
        "my_mse = mean_squared_error(myLR.predict(X_test_new), y_test)\n",
        "print(f'\\n\\n SK: {[LR.coef_]}, \\n\\n MY: {myLR.intercept, myLR.theta} sklearn mse: {sklasso_mse} my_mse: {my_mse}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgO7mEsqhoW3",
        "outputId": "3149d00b-3ab0-4290-cec4-0172067364d7"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-53-674024d7e161>:69: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  self.update_steps[epoch, column] = theta_k\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " SK: [array([  0.        ,  -0.        , 471.13142551, 115.97936366,\n",
            "         0.        ,   0.        , -26.11282509,   0.        ])], \n",
            "\n",
            " MY: (None, array([[153.1279379 ],\n",
            "       [  0.        ],\n",
            "       [  0.        ],\n",
            "       [471.12994968],\n",
            "       [115.97926556],\n",
            "       [  0.        ],\n",
            "       [  0.        ],\n",
            "       [-26.11335483],\n",
            "       [  0.        ]])) sklearn mse: 3930.7268752441273 my_mse: 3930.7286010752905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sBzKuB46h9X3"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t17nehqQjZak"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}